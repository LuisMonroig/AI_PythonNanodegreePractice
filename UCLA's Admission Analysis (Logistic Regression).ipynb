{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on UCLA's Admission Data \n",
    "\n",
    "\n",
    "<img src = \"UCLA_image.png\" alt = \"UCLA Logo\" length = 250 width = 250 />\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the logistic regression algorithm to develop a basic Neural Network that accurately determines if a student is accepted to UCLA or not. The features being used will be the GRE scores of the students, the GPA of the students, and the student rank (i.e. Explanation of Rank). \n",
    "\n",
    "At first, we will see logistic regression applied using only 1 feature: the gpa. After we complete this algorthm, and we make sure with visualizations that the algorithm is working correctly, we will use all three features and implement a multifeature logistic regression. We will always mean normalization for feature scaling and regularization to avoid overfitting. \n",
    "\n",
    "Since the data has 400 values we as usual will use 80% percent of the data(i.e. 400(.8) = 320 rows) for trainning and the other 20%(i.e.80 rows).\n",
    "\n",
    "Let us start by importing the most important libraries, definning the sigmoid function and converting the data to the ideal format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   admit   400 non-null    int64  \n",
      " 1   gre     400 non-null    int64  \n",
      " 2   gpa     400 non-null    float64\n",
      " 3   rank    400 non-null    int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 12.6 KB\n",
      "None\n",
      "Index(['admit', 'gre', 'gpa', 'rank'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#defining the sigmoid function for logistic regression as sig\n",
    "sig = lambda X: 1/(1+np.exp(-X))\n",
    "\n",
    "UCLA_data = pd.read_csv('/users/luise/desktop/Jupyter_Notebooks/UCLA_Admissions_Data.csv')\n",
    "\n",
    "\n",
    "print(UCLA_data.info())\n",
    "print(UCLA_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  rank\n",
       "0      0  380  3.61     3\n",
       "1      1  660  3.67     3\n",
       "2      1  800  4.00     1\n",
       "3      1  640  3.19     4\n",
       "4      0  520  2.93     4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UCLA_data.head() #print jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>2.84</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>1</td>\n",
       "      <td>340</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>1</td>\n",
       "      <td>780</td>\n",
       "      <td>3.63</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1</td>\n",
       "      <td>480</td>\n",
       "      <td>3.71</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>3.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit  gre   gpa  rank\n",
       "0        0  380  3.61     3\n",
       "1        1  660  3.67     3\n",
       "2        1  800  4.00     1\n",
       "3        1  640  3.19     4\n",
       "4        0  520  2.93     4\n",
       "..     ...  ...   ...   ...\n",
       "315      1  300  2.84     2\n",
       "316      1  340  3.00     2\n",
       "317      1  780  3.63     4\n",
       "318      1  480  3.71     4\n",
       "319      0  540  3.28     1\n",
       "\n",
       "[320 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UCLA_trainning_data = UCLA_data.loc[np.arange(320)] #choosing the first 320 as trainning data set\n",
    "#UCLA_test_data = UCLA_data.loc[np.arange(320,400)]\n",
    "\n",
    "UCLA_trainning_data #print jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will now normalize the data using mean normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.35732759,  0.12789152,  0.15416667],\n",
       "       [ 1.        ,  0.12543103,  0.16237428,  0.15416667],\n",
       "       [ 1.        ,  0.36681034,  0.35202945, -0.5125    ],\n",
       "       ...,\n",
       "       [ 1.        ,  0.33232759,  0.13938578,  0.4875    ],\n",
       "       [ 1.        , -0.18491379,  0.18536279,  0.4875    ],\n",
       "       [ 1.        , -0.08146552, -0.06176365, -0.5125    ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we'll redefine/reassign to X to use as matrix of multiple features\n",
    "X = np.hstack((np.ones((320,1)), UCLA_trainning_data[['gre','gpa','rank']].values.reshape(320,3)))\n",
    "\n",
    "\n",
    "\n",
    "x_max = np.array([np.max(X[:,i+1]) for i in range(3)])  #contains values for feature scaling; plus 1 to avoid lookup in column of ones \n",
    "x_min = np.array([np.min(X[:,i+1]) for i in range(3)])  #contains values for feature scaling; plus 1 to avoid lookup in column of ones\n",
    "x_mean = np.array([np.mean(X[:,i+1]) for i in range(3)])#contains means for feature scaling; plus 1 to avoid average column of ones\n",
    "\n",
    "# implementing mean normalization\n",
    "normalized_trainning_data = X #initializing new variable\n",
    "\n",
    "normalized_trainning_data[:,1] = (X[:,1] - x_mean[0])/(x_max[0]-x_min[0])\n",
    "normalized_trainning_data[:,2] = (X[:,2] - x_mean[1])/(x_max[1]-x_min[1])\n",
    "normalized_trainning_data[:,3] = (X[:,3] - x_mean[2])/(x_max[2]-x_min[2])\n",
    "\n",
    "normalized_trainning_data #print jupyter # in an np array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on the different types of feature scaling tecchniques we recomend the following site: [Feature Scaling for Machine Learning](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.315625  ]\n",
      " [ 0.2486074 ]\n",
      " [ 0.28843583]\n",
      " [-0.34029349]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.44158907, 0.44270264, 0.44338698, 0.44376325, 0.44392505,\n",
       "       0.4439411 , 0.44386038, 0.44371711, 0.44353489, 0.44332983,\n",
       "       0.4431128 , 0.44289107, 0.44266945, 0.44245107, 0.44223791,\n",
       "       0.44203117, 0.44183156, 0.4416394 , 0.44145482, 0.44127777,\n",
       "       0.44110813, 0.4409457 , 0.44079024, 0.44064151, 0.44049925,\n",
       "       0.4403632 , 0.44023309, 0.44010868, 0.43998973, 0.43987599,\n",
       "       0.43976724, 0.43966326, 0.43956384, 0.43946879, 0.43937791,\n",
       "       0.43929103, 0.43920796, 0.43912855, 0.43905263, 0.43898006,\n",
       "       0.43891069, 0.43884438, 0.438781  , 0.43872042, 0.43866254,\n",
       "       0.43860722, 0.43855436, 0.43850387, 0.43845563, 0.43840955,\n",
       "       0.43836554, 0.43832351, 0.43828338, 0.43824508, 0.43820851,\n",
       "       0.43817361, 0.43814031, 0.43810855, 0.43807825, 0.43804935,\n",
       "       0.43802181, 0.43799555, 0.43797053, 0.43794669, 0.43792399,\n",
       "       0.43790238, 0.43788181, 0.43786223, 0.43784361, 0.43782591,\n",
       "       0.43780908, 0.4377931 , 0.43777792, 0.43776351, 0.43774984,\n",
       "       0.43773687, 0.43772458, 0.43771294, 0.43770193, 0.4376915 ,\n",
       "       0.43768165, 0.43767234, 0.43766355, 0.43765526, 0.43764745,\n",
       "       0.43764009, 0.43763317, 0.43762667, 0.43762057, 0.43761485,\n",
       "       0.4376095 , 0.4376045 , 0.43759983, 0.43759548, 0.43759143,\n",
       "       0.43758767, 0.4375842 , 0.43758099, 0.43757803, 0.43757531,\n",
       "       0.43757283, 0.43757056, 0.4375685 , 0.43756665, 0.43756498,\n",
       "       0.4375635 , 0.43756218, 0.43756104, 0.43756005, 0.4375592 ,\n",
       "       0.4375585 , 0.43755794, 0.4375575 , 0.43755718, 0.43755698,\n",
       "       0.43755689, 0.4375569 , 0.43755701, 0.43755721, 0.4375575 ,\n",
       "       0.43755787, 0.43755832, 0.43755884, 0.43755944, 0.43756009,\n",
       "       0.43756082, 0.43756159, 0.43756243, 0.43756331, 0.43756424,\n",
       "       0.43756522, 0.43756624, 0.4375673 , 0.43756839, 0.43756952,\n",
       "       0.43757068, 0.43757186, 0.43757308, 0.43757432, 0.43757558,\n",
       "       0.43757686, 0.43757816, 0.43757947, 0.4375808 , 0.43758215,\n",
       "       0.4375835 , 0.43758487, 0.43758625, 0.43758763, 0.43758902,\n",
       "       0.43759041, 0.43759181, 0.43759321, 0.43759462, 0.43759602,\n",
       "       0.43759742, 0.43759883, 0.43760023, 0.43760163, 0.43760302,\n",
       "       0.43760441, 0.4376058 , 0.43760718, 0.43760855, 0.43760992,\n",
       "       0.43761129, 0.43761264, 0.43761399, 0.43761532, 0.43761665,\n",
       "       0.43761797, 0.43761928, 0.43762059, 0.43762188, 0.43762316,\n",
       "       0.43762443, 0.43762569, 0.43762693, 0.43762817, 0.43762939,\n",
       "       0.43763061, 0.43763181, 0.437633  , 0.43763418, 0.43763534,\n",
       "       0.43763649, 0.43763763, 0.43763876, 0.43763988, 0.43764098,\n",
       "       0.43764207, 0.43764315, 0.43764422, 0.43764527, 0.43764631,\n",
       "       0.43764734, 0.43764835, 0.43764935, 0.43765034, 0.43765132,\n",
       "       0.43765229, 0.43765324, 0.43765418, 0.43765511, 0.43765602,\n",
       "       0.43765693, 0.43765782, 0.4376587 , 0.43765957, 0.43766042,\n",
       "       0.43766127, 0.4376621 , 0.43766292, 0.43766373, 0.43766453,\n",
       "       0.43766532, 0.43766609, 0.43766686, 0.43766761, 0.43766835,\n",
       "       0.43766908, 0.43766981, 0.43767052, 0.43767122, 0.43767191,\n",
       "       0.43767259, 0.43767326, 0.43767392, 0.43767457, 0.43767521,\n",
       "       0.43767584, 0.43767646, 0.43767707, 0.43767767, 0.43767827,\n",
       "       0.43767885, 0.43767943, 0.43767999, 0.43768055, 0.4376811 ,\n",
       "       0.43768164, 0.43768217, 0.43768269, 0.43768321, 0.43768372,\n",
       "       0.43768422, 0.43768471, 0.43768519, 0.43768567, 0.43768614,\n",
       "       0.4376866 , 0.43768705, 0.4376875 , 0.43768794, 0.43768837,\n",
       "       0.4376888 , 0.43768922, 0.43768963, 0.43769004, 0.43769044,\n",
       "       0.43769083, 0.43769122, 0.4376916 , 0.43769197, 0.43769234,\n",
       "       0.4376927 , 0.43769306, 0.43769341, 0.43769376, 0.4376941 ,\n",
       "       0.43769443, 0.43769476, 0.43769508, 0.4376954 , 0.43769571,\n",
       "       0.43769602, 0.43769633, 0.43769662, 0.43769692, 0.43769721,\n",
       "       0.43769749, 0.43769777, 0.43769804, 0.43769831, 0.43769858,\n",
       "       0.43769884, 0.4376991 , 0.43769935, 0.4376996 , 0.43769984,\n",
       "       0.43770008, 0.43770032, 0.43770055, 0.43770078, 0.43770101,\n",
       "       0.43770123, 0.43770145, 0.43770166, 0.43770187, 0.43770208,\n",
       "       0.43770229, 0.43770249, 0.43770269, 0.43770288, 0.43770307,\n",
       "       0.43770326, 0.43770344, 0.43770363, 0.43770381, 0.43770398,\n",
       "       0.43770416, 0.43770433, 0.43770449, 0.43770466, 0.43770482,\n",
       "       0.43770498, 0.43770514, 0.43770529, 0.43770545, 0.43770559,\n",
       "       0.43770574, 0.43770589, 0.43770603, 0.43770617, 0.43770631,\n",
       "       0.43770644, 0.43770658, 0.43770671, 0.43770684, 0.43770697,\n",
       "       0.43770709, 0.43770721, 0.43770734, 0.43770745, 0.43770757,\n",
       "       0.43770769, 0.4377078 , 0.43770791, 0.43770802, 0.43770813,\n",
       "       0.43770824, 0.43770834, 0.43770845, 0.43770855, 0.43770865,\n",
       "       0.43770875, 0.43770884, 0.43770894, 0.43770903, 0.43770912,\n",
       "       0.43770922, 0.43770931, 0.43770939, 0.43770948, 0.43770957,\n",
       "       0.43770965, 0.43770973, 0.43770981, 0.43770989, 0.43770997,\n",
       "       0.43771005, 0.43771013, 0.4377102 , 0.43771028, 0.43771035,\n",
       "       0.43771042, 0.43771049, 0.43771056, 0.43771063, 0.4377107 ,\n",
       "       0.43771076, 0.43771083, 0.4377109 , 0.43771096, 0.43771102,\n",
       "       0.43771108, 0.43771114, 0.4377112 , 0.43771126, 0.43771132,\n",
       "       0.43771138, 0.43771143, 0.43771149, 0.43771154, 0.4377116 ,\n",
       "       0.43771165, 0.4377117 , 0.43771175, 0.4377118 , 0.43771185,\n",
       "       0.4377119 , 0.43771195, 0.437712  , 0.43771205, 0.43771209,\n",
       "       0.43771214, 0.43771218, 0.43771223, 0.43771227, 0.43771232,\n",
       "       0.43771236, 0.4377124 , 0.43771244, 0.43771248, 0.43771252,\n",
       "       0.43771256, 0.4377126 , 0.43771264, 0.43771268, 0.43771271,\n",
       "       0.43771275, 0.43771279, 0.43771282, 0.43771286, 0.43771289,\n",
       "       0.43771293, 0.43771296, 0.43771299, 0.43771303, 0.43771306,\n",
       "       0.43771309, 0.43771312, 0.43771315, 0.43771318, 0.43771321,\n",
       "       0.43771324, 0.43771327, 0.4377133 , 0.43771333, 0.43771336,\n",
       "       0.43771339, 0.43771341, 0.43771344, 0.43771347, 0.43771349,\n",
       "       0.43771352, 0.43771354, 0.43771357, 0.43771359, 0.43771362,\n",
       "       0.43771364, 0.43771367, 0.43771369, 0.43771371, 0.43771374,\n",
       "       0.43771376, 0.43771378, 0.4377138 , 0.43771383, 0.43771385,\n",
       "       0.43771387, 0.43771389, 0.43771391, 0.43771393, 0.43771395,\n",
       "       0.43771397, 0.43771399, 0.43771401, 0.43771403, 0.43771405,\n",
       "       0.43771407, 0.43771408, 0.4377141 , 0.43771412, 0.43771414,\n",
       "       0.43771416, 0.43771417, 0.43771419, 0.43771421, 0.43771422,\n",
       "       0.43771424, 0.43771426, 0.43771427, 0.43771429, 0.4377143 ,\n",
       "       0.43771432, 0.43771433, 0.43771435, 0.43771436, 0.43771438,\n",
       "       0.43771439, 0.43771441, 0.43771442, 0.43771443, 0.43771445,\n",
       "       0.43771446, 0.43771448, 0.43771449, 0.4377145 , 0.43771452,\n",
       "       0.43771453, 0.43771454, 0.43771455, 0.43771457, 0.43771458,\n",
       "       0.43771459, 0.4377146 , 0.43771461, 0.43771463, 0.43771464,\n",
       "       0.43771465, 0.43771466, 0.43771467, 0.43771468, 0.43771469,\n",
       "       0.4377147 , 0.43771471, 0.43771472, 0.43771473, 0.43771474,\n",
       "       0.43771475, 0.43771476, 0.43771477, 0.43771478, 0.43771479,\n",
       "       0.4377148 , 0.43771481, 0.43771482, 0.43771483, 0.43771484,\n",
       "       0.43771485, 0.43771486, 0.43771487, 0.43771488, 0.43771489,\n",
       "       0.43771489, 0.4377149 , 0.43771491, 0.43771492, 0.43771493,\n",
       "       0.43771494, 0.43771494, 0.43771495, 0.43771496, 0.43771497,\n",
       "       0.43771497, 0.43771498, 0.43771499, 0.437715  , 0.437715  ,\n",
       "       0.43771501, 0.43771502, 0.43771503, 0.43771503, 0.43771504,\n",
       "       0.43771505, 0.43771505, 0.43771506, 0.43771507, 0.43771507,\n",
       "       0.43771508, 0.43771509, 0.43771509, 0.4377151 , 0.43771511,\n",
       "       0.43771511, 0.43771512, 0.43771512, 0.43771513, 0.43771514,\n",
       "       0.43771514, 0.43771515, 0.43771515, 0.43771516, 0.43771516,\n",
       "       0.43771517, 0.43771518, 0.43771518, 0.43771519, 0.43771519,\n",
       "       0.4377152 , 0.4377152 , 0.43771521, 0.43771521, 0.43771522,\n",
       "       0.43771522, 0.43771523, 0.43771523, 0.43771524, 0.43771524,\n",
       "       0.43771525, 0.43771525, 0.43771526, 0.43771526, 0.43771527,\n",
       "       0.43771527, 0.43771528, 0.43771528, 0.43771528, 0.43771529,\n",
       "       0.43771529, 0.4377153 , 0.4377153 , 0.43771531, 0.43771531,\n",
       "       0.43771531, 0.43771532, 0.43771532, 0.43771533, 0.43771533,\n",
       "       0.43771533, 0.43771534, 0.43771534, 0.43771535, 0.43771535,\n",
       "       0.43771535, 0.43771536, 0.43771536, 0.43771536, 0.43771537,\n",
       "       0.43771537, 0.43771537, 0.43771538, 0.43771538, 0.43771538,\n",
       "       0.43771539, 0.43771539, 0.43771539, 0.4377154 , 0.4377154 ,\n",
       "       0.4377154 , 0.43771541, 0.43771541, 0.43771541, 0.43771542,\n",
       "       0.43771542, 0.43771542, 0.43771543, 0.43771543, 0.43771543,\n",
       "       0.43771544, 0.43771544, 0.43771544, 0.43771544, 0.43771545,\n",
       "       0.43771545, 0.43771545, 0.43771546, 0.43771546, 0.43771546,\n",
       "       0.43771546, 0.43771547, 0.43771547, 0.43771547, 0.43771547,\n",
       "       0.43771548, 0.43771548, 0.43771548, 0.43771548, 0.43771549,\n",
       "       0.43771549, 0.43771549, 0.43771549, 0.4377155 , 0.4377155 ,\n",
       "       0.4377155 , 0.4377155 , 0.43771551, 0.43771551, 0.43771551,\n",
       "       0.43771551, 0.43771551, 0.43771552, 0.43771552, 0.43771552,\n",
       "       0.43771552, 0.43771552, 0.43771553, 0.43771553, 0.43771553,\n",
       "       0.43771553, 0.43771553, 0.43771554, 0.43771554, 0.43771554,\n",
       "       0.43771554, 0.43771554, 0.43771555, 0.43771555, 0.43771555,\n",
       "       0.43771555, 0.43771555, 0.43771556, 0.43771556, 0.43771556,\n",
       "       0.43771556, 0.43771556, 0.43771556, 0.43771557, 0.43771557,\n",
       "       0.43771557, 0.43771557, 0.43771557, 0.43771557, 0.43771558,\n",
       "       0.43771558, 0.43771558, 0.43771558, 0.43771558, 0.43771558,\n",
       "       0.43771559, 0.43771559, 0.43771559, 0.43771559, 0.43771559,\n",
       "       0.43771559, 0.4377156 , 0.4377156 , 0.4377156 , 0.4377156 ,\n",
       "       0.4377156 , 0.4377156 , 0.4377156 , 0.4377156 , 0.43771561,\n",
       "       0.43771561, 0.43771561, 0.43771561, 0.43771561, 0.43771561,\n",
       "       0.43771561, 0.43771562, 0.43771562, 0.43771562, 0.43771562,\n",
       "       0.43771562, 0.43771562, 0.43771562, 0.43771562, 0.43771563,\n",
       "       0.43771563, 0.43771563, 0.43771563, 0.43771563, 0.43771563,\n",
       "       0.43771563, 0.43771563, 0.43771563, 0.43771564, 0.43771564,\n",
       "       0.43771564, 0.43771564, 0.43771564, 0.43771564, 0.43771564,\n",
       "       0.43771564, 0.43771564, 0.43771565, 0.43771565, 0.43771565,\n",
       "       0.43771565, 0.43771565, 0.43771565, 0.43771565, 0.43771565,\n",
       "       0.43771565, 0.43771565, 0.43771565, 0.43771566, 0.43771566,\n",
       "       0.43771566, 0.43771566, 0.43771566, 0.43771566, 0.43771566,\n",
       "       0.43771566, 0.43771566, 0.43771566, 0.43771566, 0.43771567,\n",
       "       0.43771567, 0.43771567, 0.43771567, 0.43771567, 0.43771567,\n",
       "       0.43771567, 0.43771567, 0.43771567, 0.43771567, 0.43771567,\n",
       "       0.43771567, 0.43771568, 0.43771568, 0.43771568, 0.43771568,\n",
       "       0.43771568, 0.43771568, 0.43771568, 0.43771568, 0.43771568,\n",
       "       0.43771568, 0.43771568, 0.43771568, 0.43771568, 0.43771568,\n",
       "       0.43771569, 0.43771569, 0.43771569, 0.43771569, 0.43771569,\n",
       "       0.43771569, 0.43771569, 0.43771569, 0.43771569, 0.43771569,\n",
       "       0.43771569, 0.43771569, 0.43771569, 0.43771569, 0.43771569,\n",
       "       0.43771569, 0.4377157 , 0.4377157 , 0.4377157 , 0.4377157 ,\n",
       "       0.4377157 , 0.4377157 , 0.4377157 , 0.4377157 , 0.4377157 ,\n",
       "       0.4377157 , 0.4377157 , 0.4377157 , 0.4377157 , 0.4377157 ,\n",
       "       0.4377157 , 0.4377157 , 0.4377157 , 0.4377157 , 0.4377157 ,\n",
       "       0.43771571, 0.43771571, 0.43771571, 0.43771571, 0.43771571,\n",
       "       0.43771571, 0.43771571, 0.43771571, 0.43771571, 0.43771571,\n",
       "       0.43771571, 0.43771571, 0.43771571, 0.43771571, 0.43771571,\n",
       "       0.43771571, 0.43771571, 0.43771571, 0.43771571, 0.43771571,\n",
       "       0.43771571, 0.43771571, 0.43771572, 0.43771572, 0.43771572,\n",
       "       0.43771572, 0.43771572, 0.43771572, 0.43771572, 0.43771572,\n",
       "       0.43771572, 0.43771572, 0.43771572, 0.43771572, 0.43771572,\n",
       "       0.43771572, 0.43771572, 0.43771572, 0.43771572, 0.43771572,\n",
       "       0.43771572, 0.43771572, 0.43771572, 0.43771572, 0.43771572,\n",
       "       0.43771572, 0.43771572, 0.43771572, 0.43771572, 0.43771573,\n",
       "       0.43771573, 0.43771573, 0.43771573, 0.43771573, 0.43771573,\n",
       "       0.43771573, 0.43771573, 0.43771573, 0.43771573, 0.43771573,\n",
       "       0.43771573, 0.43771573, 0.43771573, 0.43771573, 0.43771573,\n",
       "       0.43771573, 0.43771573, 0.43771573, 0.43771573, 0.43771573,\n",
       "       0.43771573, 0.43771573, 0.43771573, 0.43771573, 0.43771573,\n",
       "       0.43771573, 0.43771573, 0.43771573, 0.43771573, 0.43771573,\n",
       "       0.43771573, 0.43771573, 0.43771573, 0.43771573, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771574, 0.43771574, 0.43771574, 0.43771574, 0.43771574,\n",
       "       0.43771575, 0.43771575, 0.43771575, 0.43771575, 0.43771575,\n",
       "       0.43771575, 0.43771575, 0.43771575, 0.43771575, 0.43771575,\n",
       "       0.43771575, 0.43771575, 0.43771575, 0.43771575, 0.43771575,\n",
       "       0.43771575, 0.43771575, 0.43771575, 0.43771575, 0.43771575,\n",
       "       0.43771575, 0.43771575, 0.43771575, 0.43771575, 0.43771575,\n",
       "       0.43771575, 0.43771575, 0.43771575, 0.43771575, 0.43771575,\n",
       "       0.43771575, 0.43771575, 0.43771575, 0.43771575, 0.43771575])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.random.rand(4,1)  #setting random theta values\n",
    "y = UCLA_trainning_data['admit'].values.reshape(320,1)  # output variable \n",
    "\n",
    "num_training_eg = len(y) #number of training example\n",
    "logistic_error_vector = np.array([])    #declaring nullarray of Mean Square Errors for future visualizations\n",
    "\n",
    "learning_rate = 0.001      #for gradient descent\n",
    "\n",
    "\n",
    "for iteration in range(1000):\n",
    "    \n",
    "    y_hat = np.matmul(normalized_trainning_data,theta)\n",
    "    \n",
    "    cost_vector = y*np.log(sig(y_hat)) + (1-y)*np.log(sig(1 - y_hat))\n",
    "\n",
    "    #logistic error(i.e. error of every trainnin example gives particular weigths) \n",
    "    logistic_error = -1*(1/num_training_eg)*np.sum(cost_vector)\n",
    "    logistic_error_vector = np.append(logistic_error_vector,logistic_error) #for future vizualization\n",
    "    \n",
    "    for index in range(len(cost_vector)):    \n",
    "        theta[0,0] -= learning_rate*(y_hat[index]-y[index]) #updating the bias unit \n",
    "        theta[1,0] -= learning_rate*(y_hat[index]-y[index])*normalized_trainning_data[index,1]\n",
    "        theta[2,0] -= learning_rate*(y_hat[index]-y[index])*normalized_trainning_data[index,2]\n",
    "        theta[3,0] -= learning_rate*(y_hat[index]-y[index])*normalized_trainning_data[index,3]\n",
    "        \n",
    "print(theta)\n",
    "logistic_error_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43755688826033734"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(logistic_error_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17f3b534048>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting Logistic Error function \n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Logistic Error\")\n",
    "#plt.tittle(\"Logistic Error per Iteration\")\n",
    "plt.plot(np.arange(len(logistic_error_vector)),logistic_error_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "The hyperplane obtanied is given by the following formula: f(x1,x2,x3) = -0.34029349445260665 x3 + 0.28843582601698 x2 + 0.24860739848854474 x1 + 0.31562499999999927.\n"
     ]
    }
   ],
   "source": [
    "print(theta.shape)\n",
    "print(\"The hyperplane obtanied is given by the following formula: f(x1,x2,x3) = {} x3 + {} x2 + {} x1 + {}.\".format(theta[3][0],theta[2][0],theta[1][0],theta[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "my_range() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-aab837d6c7be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmy_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: my_range() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "def my_range(x):\n",
    "    i = 0\n",
    "    while i < x:\n",
    "        yield i\n",
    "        i += 1\n",
    "my_range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
